{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55u4PzYDT2vd"
   },
   "source": [
    "**miniRaVAEn** = code with minimal dependencies on large libraries or codebases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import fsspec\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from abc import abstractmethod\n",
    "from typing import List, Any, Dict, Tuple\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def cosine_distance_score(mu1: Tensor, mu2: Tensor):\n",
    "    return 1-cosine_similarity(mu1, mu2)\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLUjq8-OTcjt"
   },
   "outputs": [],
   "source": [
    "PATH_model_weights = \"content/last.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9It6nxQFUZBO"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### title Init data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_tile_indices_from_folder(settings_dataset):\n",
    "    path = settings_dataset[\"data_base_path\"]\n",
    "\n",
    "    isDirectory = os.path.isdir(path)\n",
    "    if isDirectory:\n",
    "        # A directory, load all tifs inside\n",
    "        allfiles = glob.glob(path+\"/*.tif\")\n",
    "        allfiles.sort()\n",
    "    elif \".tif\" in path:\n",
    "        # A single file, load that one directly\n",
    "        allfiles = [path]\n",
    "\n",
    "    tiles = []\n",
    "\n",
    "    for idx,filename in enumerate(allfiles):\n",
    "\n",
    "        tiles_from_file = file_to_tiles_indices(filename, settings_dataset,\n",
    "            tile_px_size = settings_dataset[\"tile_px_size\"], tile_overlap_px = settings_dataset[\"tile_overlap_px\"],\n",
    "            include_last_row_colum_extra_tile = settings_dataset[\"include_last_row_colum_extra_tile\"])\n",
    "\n",
    "        tiles += tiles_from_file\n",
    "        print(idx, filename, \"loaded\", len(tiles_from_file), \"tiles.\")\n",
    "\n",
    "\n",
    "    print(\"Loaded:\", len(tiles), \"total tile indices\")\n",
    "    return tiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_tiles_indices(filename, settings, tile_px_size = 128, tile_overlap_px = 4,\n",
    "                          include_last_row_colum_extra_tile = True):\n",
    "    \"\"\"\n",
    "    Opens one tif file and extracts all tiles (given tile size and overlap).\n",
    "    Returns list of indices to the tile (to postpone in memory loading).\n",
    "    \"\"\"\n",
    "\n",
    "    with rasterio.open(filename) as src:\n",
    "        filename_shape = src.height, src.width\n",
    "\n",
    "    data_h, data_w = filename_shape\n",
    "    if data_h < tile_px_size or data_w < tile_px_size:\n",
    "        # print(\"skipping, too small!\")\n",
    "        return []\n",
    "\n",
    "    h_tiles_n = int(np.floor((data_h-tile_overlap_px) / (tile_px_size-tile_overlap_px)))\n",
    "    w_tiles_n = int(np.floor((data_w-tile_overlap_px) / (tile_px_size-tile_overlap_px)))\n",
    "\n",
    "    tiles = []\n",
    "    tiles_X = []\n",
    "    tiles_Y = []\n",
    "    for h_idx in range(h_tiles_n):\n",
    "            for w_idx in range(w_tiles_n):\n",
    "                    tiles.append([w_idx * (tile_px_size-tile_overlap_px), h_idx * (tile_px_size-tile_overlap_px)])\n",
    "    if include_last_row_colum_extra_tile:\n",
    "            for w_idx in range(w_tiles_n):\n",
    "                    tiles.append([w_idx * (tile_px_size-tile_overlap_px), data_h - tile_px_size])\n",
    "            for h_idx in range(h_tiles_n):\n",
    "                    tiles.append([data_w - tile_px_size, h_idx * (tile_px_size-tile_overlap_px)])\n",
    "            tiles.append([data_w - tile_px_size, data_h - tile_px_size])\n",
    "\n",
    "    # Save file ID + corresponding tiles[]\n",
    "    tiles_indices = [[filename]+t+[tile_px_size,tile_px_size] for t in tiles]\n",
    "    return tiles_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONCE_PRINT = True\n",
    "def load_tile_idx(tile, settings):\n",
    "    \"\"\"\n",
    "    Loads tile data values from the saved indices (file and window locations).\n",
    "    \"\"\"\n",
    "    filename, x, y, w, h = tile\n",
    "\n",
    "    # load window:\n",
    "    window = rasterio.windows.Window(row_off=y, col_off=x, width=w, height=h)\n",
    "\n",
    "    if settings['bands'] is None:\n",
    "        # Load all\n",
    "        with rasterio.open(filename) as src:\n",
    "            tile_data = src.read(window=window)\n",
    "    else:\n",
    "        bands = [b+1 for b in settings['bands']]\n",
    "\n",
    "        global ONCE_PRINT\n",
    "        if ONCE_PRINT:\n",
    "            print(\"DEBUG - loaded bands\",bands)\n",
    "            ONCE_PRINT = False\n",
    "        with rasterio.open(filename) as src:\n",
    "            tile_data = src.read(bands, window=window)\n",
    "\n",
    "    tile_data = np.float32(tile_data)\n",
    "    if settings['nan_to_num']:\n",
    "        tile_data = np.nan_to_num(tile_data)\n",
    "\n",
    "    dummy = np.zeros_like(tile_data)\n",
    "    return tile_data, dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataNormalizerLogManual():\n",
    "\n",
    "    def __init__(self, settings):\n",
    "        self.settings_dataset = settings[\"dataset\"]\n",
    "        self.normalization_parameters = None\n",
    "\n",
    "    def setup(self, data_module):\n",
    "        self.BANDS_S2_BRIEF = [\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\",\"B8\"]\n",
    "        self.RESCALE_PARAMS = {\n",
    "            \"B1\" : {  \"x0\": band_params['b0_x0'],\n",
    "                      \"x1\": band_params['b0_x1'],\n",
    "                      \"y0\": -1,\n",
    "                      \"y1\": 1,\n",
    "            },\n",
    "            \"B2\" : {  \"x0\": band_params['b1_x0'],\n",
    "                      \"x1\": band_params['b1_x1'],\n",
    "                      \"y0\": -1,\n",
    "                      \"y1\": 1,\n",
    "            },\n",
    "            \"B3\" : {  \"x0\": band_params['b2_x0'],\n",
    "                      \"x1\": band_params['b2_x1'],\n",
    "                      \"y0\": -1,\n",
    "                      \"y1\": 1,\n",
    "            },\n",
    "            \"B4\" : {  \"x0\": band_params['b3_x0'],\n",
    "                      \"x1\": band_params['b3_x1'],\n",
    "                      \"y0\": -1,\n",
    "                      \"y1\": 1,\n",
    "            },\n",
    "            \"B5\" : {  \"x0\": band_params['b4_x0'],\n",
    "                      \"x1\": band_params['b4_x1'],\n",
    "                      \"y0\": -1,\n",
    "                      \"y1\": 1,\n",
    "            },\n",
    "            \"B6\" : {  \"x0\": band_params['b5_x0'],\n",
    "                      \"x1\": band_params['b5_x1'],\n",
    "                      \"y0\": -1,\n",
    "                      \"y1\": 1,\n",
    "            },\n",
    "            \"B7\" : {  \"x0\": band_params['b6_x0'],\n",
    "                      \"x1\": band_params['b6_x1'],\n",
    "                      \"y0\": -1,\n",
    "                      \"y1\": 1,\n",
    "            },\n",
    "            \"B8\" : {  \"x0\": band_params['b7_x0'],\n",
    "                      \"x1\": band_params['b7_x1'],\n",
    "                      \"y0\": -1,\n",
    "                      \"y1\": 1,\n",
    "            }            \n",
    "        }\n",
    "        print(\"normalization params are manually found\")\n",
    "\n",
    "    def normalize_x(self, data):\n",
    "        bands = data.shape[0] # for example 15\n",
    "        for band_i in range(bands):\n",
    "            data_one_band = data[band_i,:,:]\n",
    "            if band_i < len(self.BANDS_S2_BRIEF):\n",
    "                # log\n",
    "                data_one_band = np.log(data_one_band)\n",
    "                data_one_band[np.isinf(data_one_band)] = np.nan\n",
    "\n",
    "                # rescale\n",
    "                r = self.RESCALE_PARAMS[self.BANDS_S2_BRIEF[band_i]]\n",
    "                x0,x1,y0,y1 = r[\"x0\"], r[\"x1\"], r[\"y0\"], r[\"y1\"]\n",
    "                data_one_band = ((data_one_band - x0) / (x1 - x0)) * (y1 - y0) + y0\n",
    "            data[band_i,:,:] = data_one_band\n",
    "        return data\n",
    "\n",
    "    def denormalize_x(self, data):\n",
    "        bands = data.shape[0] # for example 15\n",
    "        for band_i in range(bands):\n",
    "            data_one_band = data[band_i,:,:]\n",
    "            if band_i < len(self.BANDS_S2_BRIEF):\n",
    "\n",
    "                # rescale\n",
    "                r = self.RESCALE_PARAMS[self.BANDS_S2_BRIEF[band_i]]\n",
    "                x0,x1,y0,y1 = r[\"x0\"], r[\"x1\"], r[\"y0\"], r[\"y1\"]\n",
    "                data_one_band = (((data_one_band - y0) / (y1 - y0)) * (x1 - x0)) + x0\n",
    "\n",
    "\n",
    "                # undo log\n",
    "                data_one_band = np.exp(data_one_band)\n",
    "\n",
    "            data[band_i,:,:] = data_one_band\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Dataset:\n",
    "\n",
    "class TileDataset(Dataset):\n",
    "    # Main class that holds a dataset with smaller tiles originally extracted from larger geotiff files\n",
    "    # Minimal impact on memory, loads actual data of x only in __getitem__ (when loading a batch of data)\n",
    "    # Additional functionality:\n",
    "    # - Load useful statistics for its tiles (such as the number of plume pixels in the label)\n",
    "    # - Filter itself using those statistics (example: keep valid tiles, or only tiles with plumes, etc...)\n",
    "    # - Spawn filtered tiles (to later make train / test / val splits ...)\n",
    "    def __init__(self, tiles, settings_dataset, data_normalizer=None):\n",
    "        self.tiles = tiles\n",
    "        self.settings_dataset = settings_dataset\n",
    "        self.data_normalizer = data_normalizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tile = self.tiles[idx]\n",
    "        # Load only when needed:\n",
    "        x,y = load_tile_idx(tile, self.settings_dataset)\n",
    "\n",
    "        if self.data_normalizer is not None:\n",
    "            x = self.data_normalizer.normalize_x(x)\n",
    "\n",
    "        x = torch.from_numpy(x)\n",
    "        y = torch.from_numpy(y)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lBLHUfuXUbhw"
   },
   "outputs": [],
   "source": [
    "# Pytorch Lightning Module\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, settings, data_normalizer):\n",
    "        super().__init__()\n",
    "        self.settings = settings\n",
    "        self.data_normalizer = data_normalizer\n",
    "\n",
    "        self.batch_size = self.settings[\"dataloader\"][\"batch_size\"]\n",
    "        self.num_workers = self.settings[\"dataloader\"][\"num_workers\"]\n",
    "\n",
    "        self.train_ratio = self.settings[\"dataloader\"][\"train_ratio\"]\n",
    "        self.validation_ratio = self.settings[\"dataloader\"][\"validation_ratio\"]\n",
    "        self.test_ratio = self.settings[\"dataloader\"][\"test_ratio\"]\n",
    "\n",
    "        self.setup_finished = False\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Could contain data download and unpacking...\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if self.setup_finished:\n",
    "            return True # to prevent double setup\n",
    "\n",
    "        tiles = load_all_tile_indices_from_folder(self.settings[\"dataset\"])\n",
    "        print(\"Altogether we have\", len(tiles), \"tiles.\")\n",
    "\n",
    "        if self.train_ratio == 1.0:\n",
    "            tiles_train = tiles\n",
    "            tiles_test = []\n",
    "            tiles_val = []\n",
    "        else:\n",
    "            tiles_train, tiles_rest = train_test_split(tiles, test_size=1 - self.train_ratio)\n",
    "            tiles_val, tiles_test = train_test_split(tiles_rest, test_size=self.test_ratio/(self.test_ratio + self.validation_ratio))\n",
    "\n",
    "        print(\"train, test, val:\",len(tiles_train), len(tiles_test), len(tiles_val))\n",
    "\n",
    "        self.train_dataset = TileDataset(tiles_train, self.settings[\"dataset\"], self.data_normalizer)\n",
    "        self.test_dataset = TileDataset(tiles_test, self.settings[\"dataset\"], self.data_normalizer)\n",
    "        self.val_dataset = TileDataset(tiles_val, self.settings[\"dataset\"], self.data_normalizer)\n",
    "\n",
    "        self.setup_finished = True\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Initializes and returns the training dataloader\"\"\"\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                            shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self, num_workers=None):\n",
    "        \"\"\"Initializes and returns the validation dataloader\"\"\"\n",
    "        num_workers = num_workers or self.num_workers\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                            shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    def test_dataloader(self, num_workers=None):\n",
    "        \"\"\"Initializes and returns the test dataloader\"\"\"\n",
    "        num_workers = num_workers or self.num_workers\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size,\n",
    "                            shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS = [0,1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0odH3fiXUozX"
   },
   "outputs": [],
   "source": [
    "settings = {'dataloader': {\n",
    "                'batch_size': 8,\n",
    "                'num_workers': 4,\n",
    "                'train_ratio': 1.00,\n",
    "                'validation_ratio': 0.00,\n",
    "                'test_ratio': 0.00,\n",
    "            },\n",
    "            'dataset': {\n",
    "                #'data_base_path': 'Gandahar_Mkt_after_4b',\n",
    "                #'data_base_path': 'El_Fasher_after_4b',\n",
    "                #'data_base_path': 'Muqrin_after_4b',\n",
    "                #'data_base_path': 'Jaranga_after_4b',\n",
    "                #'data_base_path': 'Sarafaya_after_4b',\n",
    "                #'data_base_path': 'Sarafaya_after_8b',\n",
    "                #'data_base_path': 'Muqrin_after_8b',\n",
    "                #'data_base_path': 'Jaranga_after_8b',\n",
    "                'data_base_path': 'El_Fasher_after_8b',\n",
    "                #'data_base_path': 'Gandahar_Mkt_after_8b',\n",
    "\n",
    "                #'data_base_path': 'Babanusa_after_4b',\n",
    "                #'data_base_path': 'single_scene_after',\n",
    "                #'data_base_path': 'test_scene_after',\n",
    "                #'data_base_path': 'test_scene_after_EG_v5',\n",
    "                #'data_base_path': 'El_Fasher_after_v1',\n",
    "                #'data_base_path': 'Al_Me_Eliq_after',\n",
    "                #'data_base_path': 'Al_Ezayba_after_8b',\n",
    "                #'data_base_path': 'Al_Ezayba_after_4b',\n",
    "                #'data_base_path': 'Babanusa_after_8b',\n",
    "                #'data_base_path': 'Muqrin_after_8b',\n",
    "                #'data_base_path': 'Jaranga_after_8b',\n",
    "                'bands': BANDS,\n",
    "                'tile_px_size': 32,\n",
    "                'tile_overlap_px': 0,\n",
    "                'include_last_row_colum_extra_tile': False,\n",
    "                'nan_to_num': False,\n",
    "             },\n",
    "            'normalizer': DataNormalizerLogManual,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "389lhqzNWKvZ",
    "outputId": "f744b8b7-a26e-4a3e-b7fc-816a4e6520ad"
   },
   "outputs": [],
   "source": [
    "data_normalizer = settings[\"normalizer\"](settings)\n",
    "print(\"loaded data_normalizer\")\n",
    "\n",
    "data_module_after = DataModule(settings, data_normalizer)\n",
    "data_module_after.setup()\n",
    "data_normalizer.setup(data_module_after)\n",
    "len_train_ds_after = len(data_module_after.val_dataloader())\n",
    "\n",
    "settings_before = settings.copy()\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Gandahar_Mkt_before_4b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'El_Fasher_before_4b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Muqrin_before_4b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Jaranga_before_4b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Sarafaya_before_4b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Sarafaya_before_8b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Muqrin_before_8b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Jaranga_before_8b'\n",
    "settings_before[\"dataset\"][\"data_base_path\"] = 'El_Fasher_before_8b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Gandahar_Mkt_before_8b'\n",
    "\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Babanusa_before_4b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'El_Fasher_before_v1'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Al_Ezayba_before_8b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Al_Ezayba_before_4b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Babanusa_before_8b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Muqrin_before_8b'\n",
    "#settings_before[\"dataset\"][\"data_base_path\"] = 'Jaranga_before_8b'\n",
    "data_module_before = DataModule(settings_before, data_normalizer)\n",
    "data_module_before.setup()\n",
    "data_normalizer.setup(data_module_before)\n",
    "len_train_ds_before = len(data_module_before.val_dataloader())\n",
    "\n",
    "assert len_train_ds_after == len_train_ds_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPw6wPRCInS6",
    "outputId": "11be0524-112c-41d4-b896-ad66af48c680"
   },
   "outputs": [],
   "source": [
    "bands = 8\n",
    "#bands = 4\n",
    "for band_i in range(bands):\n",
    "    if band_i < len(data_normalizer.BANDS_S2_BRIEF):\n",
    "\n",
    "        # rescale\n",
    "        r = data_normalizer.RESCALE_PARAMS[data_normalizer.BANDS_S2_BRIEF[band_i]]\n",
    "        x0,x1,y0,y1 = r[\"x0\"], r[\"x1\"], r[\"y0\"], r[\"y1\"]\n",
    "\n",
    "        print(data_normalizer.BANDS_S2_BRIEF[band_i], \"->\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MTqts-uktZo",
    "outputId": "862ee17f-9953-4312-8d7e-d24a440168bb"
   },
   "outputs": [],
   "source": [
    "# We can also later just use the loaded tiles:\n",
    "\n",
    "after_array = []\n",
    "before_array = []\n",
    "for sample in tqdm(data_module_after.train_dataset):\n",
    "    after_array.append(sample[0])\n",
    "for sample in tqdm(data_module_before.train_dataset):\n",
    "    before_array.append(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_array = [x.numpy() for x in before_array]\n",
    "after_array = [x.numpy() for x in after_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_rRyzlqlXR6"
   },
   "outputs": [],
   "source": [
    "before_array = np.asarray(before_array)\n",
    "after_array = np.asarray(after_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NkYqV2lxGjM",
    "outputId": "2f57090a-b1d8-4875-bafe-11acdbbf5d93"
   },
   "outputs": [],
   "source": [
    "print(\"Now we have\", len(before_array),\"*\",before_array[0].shape, \"as data from the image before the event and \",len(after_array),\"*\",after_array[0].shape, \"from the image after the event.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbCmAA5Fu8Gy",
    "outputId": "c05d7fa2-8c0c-47e5-b6ff-57a362f8d786"
   },
   "outputs": [],
   "source": [
    "# this data should already be normalised:\n",
    "# min, max was obtained on a training dataset, so some samples may be outside of the -1,1, but most should be near\n",
    "sample_idx = 0\n",
    "band_inspect_idx = 0\n",
    "np.nanmin(before_array[sample_idx][band_inspect_idx]), np.nanmax(before_array[sample_idx][band_inspect_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fiCPvvXTwNw"
   },
   "source": [
    "# Make a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### @title Init model functions\n",
    "\n",
    "#### RaVAEn models as torch nn modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, batch: Tensor, *inputs: Any, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class BaseAE(BaseModel):\n",
    "    def __init__(self, visualisation_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.visualisation_channels = visualisation_channels\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> Tensor:\n",
    "        z = self.encode(torch.nan_to_num(input))\n",
    "        return self.decode(z)\n",
    "\n",
    "    def loss_function(self,\n",
    "                      input: Tensor,\n",
    "                      results: Dict,\n",
    "                      mask_invalid: bool = False,\n",
    "                      **kwargs) -> Dict:\n",
    "\n",
    "        if not mask_invalid:\n",
    "            recons_loss = F.mse_loss(results, torch.nan_to_num(input))\n",
    "        else:\n",
    "            invalid_mask = torch.isnan(input)\n",
    "            recons_loss = \\\n",
    "                F.mse_loss(results[~invalid_mask], input[~invalid_mask])\n",
    "\n",
    "        return {'loss': recons_loss, 'Reconstruction_Loss': recons_loss}\n",
    "\n",
    "    def _visualise_step(self, batch):\n",
    "        result = self.forward(batch)\n",
    "        rec_error = (batch - result).abs()\n",
    "        return batch[:, self.visualisation_channels], result[:, self.visualisation_channels], \\\n",
    "              rec_error.max(1)[0]\n",
    "\n",
    "    @property\n",
    "    def _visualisation_labels(self):\n",
    "        return [\"Input\", \"Reconstruction\", \"Rec error\"]\n",
    "    \n",
    "class BaseVAE(BaseAE):\n",
    "    def sample(self, batch_size: int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise RuntimeWarning()\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _visualise_step(self, batch):\n",
    "        result = self.forward(batch)\n",
    "        # if VAE\n",
    "        result = result[0]\n",
    "\n",
    "        rec_error = (batch - result).abs()\n",
    "        return batch[:, self.visualisation_channels], result[:, self.visualisation_channels], \\\n",
    "              rec_error.max(1)[0]   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deeper models - have parameters to change the model sizes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional block which preserves the height and width of the input image.\n",
    "\n",
    "    (convolution => [BN] => LeakyReLU) * depth\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, depth=2, activation=nn.LeakyReLU, batchnorm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for n in range(1, depth+1):\n",
    "            layers += [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)]\n",
    "            if batchnorm:\n",
    "                layers += [nn.BatchNorm2d(out_channels)]\n",
    "            if activation is not None:\n",
    "                layers +=[activation()]\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)   \n",
    "\n",
    "class ResConvBlock(ConvBlock):\n",
    "    def forward(self, x):\n",
    "        dx = self.conv_block(x)\n",
    "        return  x + dx     \n",
    "    \n",
    "class DownConv(nn.Module):\n",
    "    \"\"\"Downscaling block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, activation=nn.LeakyReLU, batchnorm=True):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)]\n",
    "        if batchnorm:\n",
    "            layers += [nn.BatchNorm2d(out_channels)]\n",
    "        if activation is not None:\n",
    "            layers +=[activation()]\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)    \n",
    "    \n",
    "class UpConv(nn.Module):\n",
    "    \"\"\"Upscaling layer with single convolution\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, upsample_method='nearest',\n",
    "                 activation=nn.LeakyReLU, batchnorm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if upsample_method in ['nearest', 'linear', 'bilinear', 'bicubic']:\n",
    "            align_corners=None if upsample_method==\"nearest\" else True\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(\n",
    "                    scale_factor=2,\n",
    "                    mode=upsample_method,\n",
    "                    align_corners=align_corners\n",
    "                ),\n",
    "                ConvBlock(in_channels, out_channels, depth=1, activation=activation, batchnorm=batchnorm)\n",
    "            )\n",
    "\n",
    "        elif upsample_method=='transpose':\n",
    "            layers = [\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels, out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1\n",
    "                ),\n",
    "            ]\n",
    "            if batchnorm:\n",
    "                layers += [nn.BatchNorm2d(out_channels)]\n",
    "            if activation is not None:\n",
    "                layers +=[activation()]\n",
    "            self.up = nn.Sequential(*layers)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Upsample method has not been implemented: {upsample_method}\"\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperVAE(BaseVAE):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape: Tuple[int],\n",
    "                 hidden_channels: List[int],\n",
    "                 latent_dim: int,\n",
    "                 extra_depth_on_scale: int,\n",
    "                 visualisation_channels,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(visualisation_channels)\n",
    "\n",
    "        assert input_shape[1]>=2**len(hidden_channels), \"Cannot have so many downscaling layers\"\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Calculate size of encoder output\n",
    "        encoder_output_width = int(input_shape[1]/(2**len(hidden_channels)))\n",
    "        encoder_output_dim = int(encoder_output_width**2 * hidden_channels[-1])\n",
    "        self.encoder_output_shape = (hidden_channels[-1], encoder_output_width, encoder_output_width)\n",
    "\n",
    "        if encoder_output_dim<latent_dim:\n",
    "            raise UserWarning(\n",
    "                f\"Encoder output dim {encoder_output_dim} is smaller than latent dim {latent_dim}.\"+\n",
    "                \"This means the bottle neck is tighter than intended.\"\n",
    "            )\n",
    "\n",
    "        in_channels = input_shape[0]\n",
    "\n",
    "        self.encoder = self._build_encoder([in_channels]+hidden_channels, extra_depth_on_scale)\n",
    "\n",
    "        self.fc_mu = nn.Linear(encoder_output_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(encoder_output_dim, latent_dim)\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, encoder_output_dim)\n",
    "\n",
    "        self.decoder = self._build_decoder(hidden_channels[::-1]+[in_channels], extra_depth_on_scale)\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_encoder(channels, extra_depth):\n",
    "        in_channels = channels[0]\n",
    "        encoder = []\n",
    "        for out_channels in channels[1:]:\n",
    "            encoder+=[\n",
    "                DownConv(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    activation=nn.LeakyReLU,\n",
    "                    batchnorm=True\n",
    "                )\n",
    "            ]\n",
    "            if extra_depth>0:\n",
    "                encoder+=[\n",
    "                    ResConvBlock(\n",
    "                        out_channels,\n",
    "                        out_channels,\n",
    "                        activation=nn.LeakyReLU,\n",
    "                        batchnorm=True,\n",
    "                        depth=extra_depth\n",
    "                    )\n",
    "                ]\n",
    "            # for next time round loop\n",
    "            in_channels = out_channels\n",
    "\n",
    "        return nn.Sequential(*encoder)\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_decoder(channels, extra_depth):\n",
    "        in_channels = channels[0]\n",
    "        decoder = []\n",
    "        up_activation = nn.LeakyReLU\n",
    "        res_activation = nn.LeakyReLU\n",
    "        for i, out_channels in enumerate(channels[1:]):\n",
    "            # if last layer use linear activation\n",
    "            is_last_layer = (i==(len(channels)-2))\n",
    "            if is_last_layer:\n",
    "                up_activation = None\n",
    "\n",
    "            decoder+=[\n",
    "                UpConv(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    upsample_method='nearest',\n",
    "                    activation=up_activation,\n",
    "                    batchnorm=not is_last_layer,\n",
    "                )\n",
    "            ]\n",
    "            if extra_depth>0:\n",
    "                decoder+=[\n",
    "                    ResConvBlock(\n",
    "                        out_channels,\n",
    "                        out_channels,\n",
    "                        activation=res_activation,\n",
    "                        batchnorm=not is_last_layer,\n",
    "                        depth=extra_depth\n",
    "                    )\n",
    "                ]\n",
    "            # for next time round loop\n",
    "            in_channels = out_channels\n",
    "\n",
    "        return nn.Sequential(*decoder)\n",
    "\n",
    "    def encode(self, input: Tensor, verbose=False) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "\n",
    "        if verbose:\n",
    "          x = input0\n",
    "          print(\"input\", x.shape)\n",
    "          for multilayer in self.encoder:\n",
    "              for layer in multilayer.conv:\n",
    "                x = layer(x)\n",
    "                print(layer,\" => it's output:\\n\", x.shape)\n",
    "\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        if verbose: print(\"result\", result.shape)\n",
    "\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        if verbose: print(\"mu\", self.fc_mu, \" => it's output:\\n\", mu.shape)\n",
    "        log_var = self.fc_var(result)\n",
    "        if verbose: print(\"log_var\", self.fc_var, \" => it's output:\\n\", log_var.shape)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, *self.encoder_output_shape)\n",
    "        result = self.decoder(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(torch.nan_to_num(input))\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return [self.decode(z), mu, log_var]\n",
    "\n",
    "    def loss_function(self, input: Tensor, results: Any, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # invalid_mask = torch.isnan(input)\n",
    "        input = torch.nan_to_num(input)\n",
    "\n",
    "        recons = results[0]\n",
    "        mu = results[1]\n",
    "        log_var = results[2]\n",
    "\n",
    "        # Account for the minibatch samples from the dataset\n",
    "        kld_weight = kwargs['M_N']\n",
    "\n",
    "        recons_loss = F.mse_loss(recons, input)\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim=1), dim=0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss,\n",
    "                'Reconstruction_Loss': recons_loss,\n",
    "                'KLD': -kld_loss}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples: int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nrtEFeJYTyKE"
   },
   "outputs": [],
   "source": [
    "# RaVAEn module to be used with pytorch lightning\n",
    "\n",
    "class Module(pl.LightningModule):\n",
    "    def __init__(self, model_cls, cfg: dict, train_cfg: dict, model_cls_args:dict) -> None:\n",
    "        super().__init__()\n",
    "        self.__dict__.update(cfg)\n",
    "        self.__dict__.update(train_cfg)\n",
    "\n",
    "        self.model = model_cls(input_shape=self.input_shape, **model_cls_args)\n",
    "\n",
    "        if hasattr(self.model, '_visualise_step'):\n",
    "            self._visualise_step = \\\n",
    "                lambda batch: self.model._visualise_step(batch[0])\n",
    "            self._visualisation_labels = self.model._visualisation_labels\n",
    "\n",
    "    def forward(self, batch: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        return self.model(batch, **kwargs)\n",
    "\n",
    "    def log_losses(self, loss, where):\n",
    "        for k in loss.keys():\n",
    "            self.log(f'{where}/{k}', loss[k], on_epoch=True, logger=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx=0):\n",
    "        batch = batch[0]\n",
    "        batch_size = batch.shape[0]\n",
    "\n",
    "        results = self.forward(batch)\n",
    "        train_loss = self.model.loss_function(batch,\n",
    "                                              results,\n",
    "                                              M_N=batch_size / self.len_train_ds,\n",
    "                                              optimizer_idx=optimizer_idx,\n",
    "                                              batch_idx=batch_idx)\n",
    "\n",
    "        self.log_losses(train_loss, 'train')\n",
    "\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, optimizer_idx=0):\n",
    "        batch = batch[0]\n",
    "        batch_size = batch.shape[0]\n",
    "\n",
    "        results = self.forward(batch)\n",
    "        val_loss = self.model.loss_function(batch,\n",
    "                                            results,\n",
    "                                            M_N=batch_size / self.len_val_ds,\n",
    "                                            optimizer_idx=optimizer_idx,\n",
    "                                            batch_idx=batch_idx)\n",
    "\n",
    "        self.log_losses(val_loss, 'valid')\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optims = []\n",
    "        scheds = []\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.lr,\n",
    "                               weight_decay=self.weight_decay)\n",
    "        optims.append(optimizer)\n",
    "\n",
    "        if hasattr(self, 'scheduler_gamma'):\n",
    "            scheduler = \\\n",
    "                optim.lr_scheduler.ExponentialLR(optims[0],\n",
    "                                                 gamma=self.scheduler_gamma)\n",
    "            scheds.append(scheduler)\n",
    "\n",
    "        if hasattr(self, 'lr2'):\n",
    "            optimizer2 = \\\n",
    "                optim.Adam(getattr(self.model, self.submodel).parameters(),\n",
    "                           lr=self.lr2)\n",
    "            optims.append(optimizer2)\n",
    "\n",
    "        # Check if another scheduler is required for the second optimizer\n",
    "        if hasattr(self, 'scheduler_gamma2'):\n",
    "            scheduler2 = \\\n",
    "                optim.lr_scheduler.ExponentialLR(optims[1],\n",
    "                                                 gamma=self.scheduler_gamma2)\n",
    "            scheds.append(scheduler2)\n",
    "\n",
    "        return optims, scheds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cA0HcosccIqd"
   },
   "source": [
    "# Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(mu1: Tensor, log_var1: Tensor, mu2: Tensor, log_var2: Tensor, reduce_axes: Tuple[int] = (-1,)):\n",
    "    \"\"\" returns KL(D_2 || D_1) assuming Gaussian distributions with diagonal covariance matrices, and taking D_1 as reference\n",
    "    ----\n",
    "    mu1, mu2, log_var1, log_var2: Tensors of sizes (..., Z) (e.g. (Z), (B, Z))\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    log_det = log_var1 - log_var2\n",
    "    trace_cov = (-log_det).exp()\n",
    "    mean_diff = (mu1 - mu2)**2 / log_var1.exp()\n",
    "    return 0.5 * ((trace_cov + mean_diff + log_det).sum(reduce_axes) - mu1.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TQo5J9yC4ud"
   },
   "outputs": [],
   "source": [
    "def twin_vae_change_score(model, x_1, x_2, verbose=False):\n",
    "    if \"VAE\" in str(model.__class__):\n",
    "        #print(x_1.dtype)\n",
    "        mu_1, log_var_1 = model.encode(x_1) # batch, latent_dim\n",
    "        mu_2, log_var_2 = model.encode(x_2) # batch, latent_dim\n",
    "\n",
    "    else:\n",
    "        assert False, \"To be implemented!\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"x_1\", type(x_1), len(x_1), x_1[0].shape) # x 256 torch.Size([3, 32, 32])\n",
    "        print(\"mu_1\", type(mu_1), len(mu_1), mu_1[0].shape) #\n",
    "        print(\"log_var_1\", type(log_var_1), len(log_var_1), log_var_1[0].shape) #\n",
    "\n",
    "    # distance = KL_divergence(mu_1, log_var_1, mu_2, log_var_2)\n",
    "    distance = cosine_distance_score(mu_1, mu_2)\n",
    "\n",
    "    if verbose: print(\"distance\", type(distance), len(distance), distance[0].shape)\n",
    "\n",
    "    # convert to numpy\n",
    "    distance = distance.detach().cpu().numpy()\n",
    "    if verbose: print(\"distance\", distance.shape)\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8DMBbqLCOCE",
    "outputId": "9b47252e-043b-46a6-f525-0c9c4918c458"
   },
   "outputs": [],
   "source": [
    "def which_device(model):\n",
    "    device = next(model.parameters()).device\n",
    "    print(\"Model is on:\", device)\n",
    "    return device\n",
    "\n",
    "#model = module.model\n",
    "#model.eval()\n",
    "\n",
    "#device = which_device(model)\n",
    "\n",
    "# We have: model.forward .encode, .decode\n",
    "\n",
    "#compare_func = twin_vae_change_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    # step 1\n",
    "    #cfg_module = {\"input_shape\": (4, 32, 32)\n",
    "    cfg_module = {\"input_shape\": (8, 32, 32),\n",
    "                  \"visualisation_channels\": [0, 1, 2],\n",
    "                  \"len_train_ds\": len_train_ds_after,\n",
    "                  \"len_val_ds\": 0,\n",
    "    }\n",
    "\n",
    "    cfg_train = {}\n",
    "\n",
    "    model_cls_args_VAE = {\n",
    "            # Using Small model:\n",
    "            \"hidden_channels\": [16, 32, 64], # number of channels after each downscale. Reversed on upscale\n",
    "            \"latent_dim\": 128,                # bottleneck size\n",
    "            \"extra_depth_on_scale\": 0,        # after each downscale and upscale, this many convolutions are applied\n",
    "            \"visualisation_channels\": cfg_module[\"visualisation_channels\"],\n",
    "    }\n",
    "\n",
    "    module = Module(DeeperVAE, cfg_module, cfg_train, model_cls_args_VAE)\n",
    "\n",
    "    hparams = {}\n",
    "    namespace = Namespace(**hparams)\n",
    "    \n",
    "    Module.load_from_checkpoint(checkpoint_path=PATH_model_weights, hparams=namespace,\n",
    "                            model_cls=DeeperVAE, train_cfg=cfg_train, model_cls_args=model_cls_args_VAE)\n",
    "    \n",
    "    # step 2\n",
    "    model = module.model\n",
    "    model.eval()\n",
    "\n",
    "#    device = which_device(model)\n",
    "\n",
    "    # We have: model.forward .encode, .decode\n",
    "\n",
    "    compare_func = twin_vae_change_score    \n",
    "    \n",
    "    # step 3\n",
    "    predicted_distances = []\n",
    "\n",
    "    # Dataloaders load it in batches - these are loaded on demand\n",
    "    # for before, after in zip(data_module_before.train_dataloader(), data_module_after.train_dataloader()):\n",
    "        # before_data = before[0]\n",
    "        # after_data = after[0]\n",
    "\n",
    "    # While iterating over the arrays loads only one by one - these are already loaded in memory\n",
    "    for before, after in zip(before_array, after_array):\n",
    "        before_data = torch.from_numpy(before).unsqueeze(0)\n",
    "        after_data = torch.from_numpy(after).unsqueeze(0)\n",
    "\n",
    "        distances = compare_func(model, before_data, after_data)\n",
    "        predicted_distances.append(distances)\n",
    "\n",
    "    return predicted_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_array[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-NUz6pri1G8"
   },
   "source": [
    "# Re-tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggrRsVmLi3eC"
   },
   "outputs": [],
   "source": [
    "def tiles2image(predicted_distances, grid_shape, overlap=0, tile_size = 32, channels = 1):\n",
    "    # predicted_distances shape of ~ N\n",
    "    image = np.zeros((channels, grid_shape[1]*tile_size, grid_shape[0]*tile_size), dtype=np.float32)\n",
    "    index = 0\n",
    "    for i in range(grid_shape[1]):\n",
    "        for j in range(grid_shape[0]):\n",
    "            tile = predicted_distances[index] * np.ones((channels, tile_size, tile_size))\n",
    "            image[:, i*tile_size:(i+1)*tile_size, j*tile_size:(j+1)*tile_size] = tile\n",
    "            index += 1\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El Fasher 8-band\n",
    "max_predicted_distances = test_model()\n",
    "max_predicted_distances = np.asarray(max_predicted_distances).flatten()\n",
    "\n",
    "max_run_num = 0\n",
    "\n",
    "for run_num in range(0,5000):\n",
    "    predicted_distances = test_model()\n",
    "    predicted_distances = np.asarray(predicted_distances).flatten()\n",
    "    if(max(predicted_distances) > max(max_predicted_distances)):\n",
    "        max_predicted_distances = predicted_distances\n",
    "        max_run_num = run_num\n",
    "grid_shape = (42, 42) # x tiles\n",
    "change_map_image = tiles2image(max_predicted_distances, grid_shape = grid_shape, overlap=0, tile_size = 32)\n",
    "\n",
    "plt.imshow(change_map_image[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Run number: {max_run_num}\")\n",
    "print(pd.DataFrame(max_predicted_distances).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gandahar market 8-band\n",
    "max_predicted_distances = test_model()\n",
    "max_predicted_distances = np.asarray(max_predicted_distances).flatten()\n",
    "\n",
    "max_run_num = 0\n",
    "\n",
    "for run_num in range(0,5000):\n",
    "    predicted_distances = test_model()\n",
    "    predicted_distances = np.asarray(predicted_distances).flatten()\n",
    "    if(max(predicted_distances) > max(max_predicted_distances)):\n",
    "        max_predicted_distances = predicted_distances\n",
    "        max_run_num = run_num\n",
    "grid_shape = (32, 32) # x tiles\n",
    "change_map_image = tiles2image(max_predicted_distances, grid_shape = grid_shape, overlap=0, tile_size = 32)\n",
    "\n",
    "plt.imshow(change_map_image[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Run number: {max_run_num}\")\n",
    "print(pd.DataFrame(max_predicted_distances).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaranga 8-band\n",
    "max_predicted_distances = test_model()\n",
    "max_predicted_distances = np.asarray(max_predicted_distances).flatten()\n",
    "\n",
    "max_run_num = 0\n",
    "\n",
    "for run_num in range(0,5000):\n",
    "    predicted_distances = test_model()\n",
    "    predicted_distances = np.asarray(predicted_distances).flatten()\n",
    "    if(max(predicted_distances) > max(max_predicted_distances)):\n",
    "        max_predicted_distances = predicted_distances\n",
    "        max_run_num = run_num\n",
    "grid_shape = (16, 16) # x tiles\n",
    "change_map_image = tiles2image(max_predicted_distances, grid_shape = grid_shape, overlap=0, tile_size = 32)\n",
    "\n",
    "plt.imshow(change_map_image[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Run number: {max_run_num}\")\n",
    "print(pd.DataFrame(max_predicted_distances).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muqrin 8-band\n",
    "max_predicted_distances = test_model()\n",
    "max_predicted_distances = np.asarray(max_predicted_distances).flatten()\n",
    "\n",
    "max_run_num = 0\n",
    "\n",
    "for run_num in range(0,5000):\n",
    "    predicted_distances = test_model()\n",
    "    predicted_distances = np.asarray(predicted_distances).flatten()\n",
    "    if(max(predicted_distances) > max(max_predicted_distances)):\n",
    "        max_predicted_distances = predicted_distances\n",
    "        max_run_num = run_num\n",
    "grid_shape = (33, 33) # x tiles\n",
    "change_map_image = tiles2image(max_predicted_distances, grid_shape = grid_shape, overlap=0, tile_size = 32)\n",
    "\n",
    "plt.imshow(change_map_image[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Run number: {max_run_num}\")\n",
    "print(pd.DataFrame(max_predicted_distances).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarafaya 8-band\n",
    "max_predicted_distances = test_model()\n",
    "max_predicted_distances = np.asarray(max_predicted_distances).flatten()\n",
    "\n",
    "max_run_num = 0\n",
    "\n",
    "for run_num in range(0,5000):\n",
    "    predicted_distances = test_model()\n",
    "    predicted_distances = np.asarray(predicted_distances).flatten()\n",
    "    if(max(predicted_distances) > max(max_predicted_distances)):\n",
    "        max_predicted_distances = predicted_distances\n",
    "        max_run_num = run_num\n",
    "grid_shape = (15, 15) # x tiles\n",
    "change_map_image = tiles2image(max_predicted_distances, grid_shape = grid_shape, overlap=0, tile_size = 32)\n",
    "\n",
    "plt.imshow(change_map_image[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Run number: {max_run_num}\")\n",
    "print(pd.DataFrame(max_predicted_distances).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarafaya 4-band, re-trained, manual normalization params, ORDER_1\n",
    "max_predicted_distances = test_model()\n",
    "max_predicted_distances = np.asarray(max_predicted_distances).flatten()\n",
    "\n",
    "max_run_num = 0\n",
    "\n",
    "for run_num in range(0,5000):\n",
    "    predicted_distances = test_model()\n",
    "    predicted_distances = np.asarray(predicted_distances).flatten()\n",
    "    if(max(predicted_distances) > max(max_predicted_distances)):\n",
    "        max_predicted_distances = predicted_distances\n",
    "        max_run_num = run_num\n",
    "grid_shape = (15, 15) # x tiles\n",
    "change_map_image = tiles2image(max_predicted_distances, grid_shape = grid_shape, overlap=0, tile_size = 32)\n",
    "\n",
    "plt.imshow(change_map_image[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Run number: {max_run_num}\")\n",
    "print(pd.DataFrame(max_predicted_distances).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaranga 4-band, re-trained, manual normalization params, ORDER_1\n",
    "max_predicted_distances = test_model()\n",
    "max_predicted_distances = np.asarray(max_predicted_distances).flatten()\n",
    "\n",
    "max_run_num = 0\n",
    "\n",
    "for run_num in range(0,5000):\n",
    "    predicted_distances = test_model()\n",
    "    predicted_distances = np.asarray(predicted_distances).flatten()\n",
    "    if(max(predicted_distances) > max(max_predicted_distances)):\n",
    "        max_predicted_distances = predicted_distances\n",
    "        max_run_num = run_num\n",
    "grid_shape = (16, 16) # x tiles\n",
    "change_map_image = tiles2image(max_predicted_distances, grid_shape = grid_shape, overlap=0, tile_size = 32)\n",
    "\n",
    "plt.imshow(change_map_image[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Run number: {max_run_num}\")\n",
    "print(pd.DataFrame(max_predicted_distances).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muqrin 4-band, re-trained, manual normalization params, ORDER_1\n",
    "max_predicted_distances = test_model()\n",
    "max_predicted_distances = np.asarray(max_predicted_distances).flatten()\n",
    "\n",
    "max_run_num = 0\n",
    "\n",
    "for run_num in range(0,5000):\n",
    "    predicted_distances = test_model()\n",
    "    predicted_distances = np.asarray(predicted_distances).flatten()\n",
    "    if(max(predicted_distances) > max(max_predicted_distances)):\n",
    "        max_predicted_distances = predicted_distances\n",
    "        max_run_num = run_num\n",
    "grid_shape = (33, 33) # x tiles\n",
    "change_map_image = tiles2image(max_predicted_distances, grid_shape = grid_shape, overlap=0, tile_size = 32)\n",
    "\n",
    "plt.imshow(change_map_image[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Run number: {max_run_num}\")\n",
    "print(pd.DataFrame(max_predicted_distances).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El Fasher 4-band, re-trained, manual normalization params, ORDER_1\n",
    "max_predicted_distances = test_model()\n",
    "max_predicted_distances = np.asarray(max_predicted_distances).flatten()\n",
    "\n",
    "max_run_num = 0\n",
    "\n",
    "for run_num in range(0,5000):\n",
    "    predicted_distances = test_model()\n",
    "    predicted_distances = np.asarray(predicted_distances).flatten()\n",
    "    if(max(predicted_distances) > max(max_predicted_distances)):\n",
    "        max_predicted_distances = predicted_distances\n",
    "        max_run_num = run_num\n",
    "grid_shape = (42, 42) # x tiles\n",
    "change_map_image = tiles2image(max_predicted_distances, grid_shape = grid_shape, overlap=0, tile_size = 32)\n",
    "\n",
    "plt.imshow(change_map_image[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Run number: {max_run_num}\")\n",
    "print(pd.DataFrame(max_predicted_distances).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gandahar market 4-band, re-trained, manual normalization params, ORDER_1\n",
    "max_predicted_distances = test_model()\n",
    "max_predicted_distances = np.asarray(max_predicted_distances).flatten()\n",
    "\n",
    "max_run_num = 0\n",
    "\n",
    "for run_num in range(0,5000):\n",
    "    predicted_distances = test_model()\n",
    "    predicted_distances = np.asarray(predicted_distances).flatten()\n",
    "    if(max(predicted_distances) > max(max_predicted_distances)):\n",
    "        max_predicted_distances = predicted_distances\n",
    "        max_run_num = run_num\n",
    "grid_shape = (32, 32) # x tiles\n",
    "change_map_image = tiles2image(max_predicted_distances, grid_shape = grid_shape, overlap=0, tile_size = 32)\n",
    "\n",
    "plt.imshow(change_map_image[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Run number: {max_run_num}\")\n",
    "print(pd.DataFrame(max_predicted_distances).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get no. of tiles per row and column of the input image\n",
    "filename='./El_Fasher_after_4b/El_Fasher_2024_05_11_after_4_b.tif'\n",
    "with rasterio.open(filename) as src:\n",
    "    filename_shape = src.height, src.width\n",
    "    print(filename_shape)\n",
    "\n",
    "tile_px_size = 32    \n",
    "data_h, data_w = filename_shape\n",
    "h_tiles_n = int(np.floor(data_h / tile_px_size))\n",
    "w_tiles_n = int(np.floor(data_w / tile_px_size)) \n",
    "(h_tiles_n,w_tiles_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_predicted_distances = test_model()\n",
    "max_predicted_distances = np.asarray(max_predicted_distances).flatten()\n",
    "\n",
    "max_run_num = 0\n",
    "\n",
    "for run_num in range(0,500):\n",
    "    predicted_distances = test_model()\n",
    "    predicted_distances = np.asarray(predicted_distances).flatten()\n",
    "    if(max(predicted_distances) > max(max_predicted_distances)):\n",
    "        max_predicted_distances = predicted_distances\n",
    "        max_run_num = run_num\n",
    "grid_shape = (32, 32) # x tiles\n",
    "change_map_image = tiles2image(max_predicted_distances, grid_shape = grid_shape, overlap=0, tile_size = 32)\n",
    "\n",
    "plt.imshow(change_map_image[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Run number: {max_run_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ravaen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
